{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import skopt\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = conv3x3(3,64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, lin=0, lout=5):\n",
    "        out = x\n",
    "        if lin < 1 and lout > -1:\n",
    "            out = self.conv1(out)\n",
    "            out = self.bn1(out)\n",
    "            out = F.relu(out)\n",
    "        if lin < 2 and lout > 0:\n",
    "            out = self.layer1(out)\n",
    "        if lin < 3 and lout > 1:\n",
    "            out = self.layer2(out)\n",
    "        if lin < 4 and lout > 2:\n",
    "            out = self.layer3(out)\n",
    "        if lin < 5 and lout > 3:\n",
    "            out = self.layer4(out)\n",
    "        if lout > 4:\n",
    "            out = F.avg_pool2d(out, 4)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(PreActBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(lr=0.1, seed=2021, batch_size=256, epoch=31, decay=1e-4)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if args.seed != 0:\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = args.lr\n",
    "    if epoch >= 10:\n",
    "        lr /= 5\n",
    "    if epoch >= 20:\n",
    "        lr /= 5\n",
    "    if epoch >= 25:\n",
    "        lr /= 5\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_baseline(epoch, trainloader, net, Loss, optimizer):\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        outputs = net(inputs)\n",
    "        loss = Loss(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def performance(epoch, net, Loss):\n",
    "    train_loss, test_loss = 0, 0\n",
    "    train_correct, test_correct = 0, 0\n",
    "    train_total, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = Loss(outputs, targets)\n",
    "            train_loss += loss.data\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = Loss(outputs, targets)\n",
    "            test_loss += loss.data\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets.data).cpu().sum()\n",
    "    if epoch % 10 == 0 or train_correct == train_total:\n",
    "        print('\\nEpoch: %d ===============================================================================' % epoch)\n",
    "        print('Train Loss: %.3f | Train Acc: %.3f%% (%d/%d) \\t Test  Loss: %.3f | Test  Acc: %.3f%% (%d/%d)' % \\\n",
    "              (train_loss/(batch_idx+1), 100.*train_correct/train_total, train_correct, train_total, \\\n",
    "               test_loss/(batch_idx+1), 100.*test_correct/test_total, test_correct, test_total))\n",
    "\n",
    "    return (train_loss/(batch_idx+1), 100.*train_correct/train_total, test_loss/(batch_idx+1), 100.*test_correct/test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 1.926 | Train Acc: 65.238% (32619/50000) \t Test  Loss: 1.028 | Test  Acc: 63.230% (6323/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.096 | Train Acc: 98.340% (49170/50000) \t Test  Loss: 0.692 | Test  Acc: 83.540% (8354/10000)\n",
      "\n",
      "Epoch: 15 ===============================================================================\n",
      "Train Loss: 0.000 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 1.006 | Test  Acc: 86.290% (8629/10000)\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "net = ResNet18()\n",
    "net.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.decay)\n",
    "writer = SummaryWriter('./performance/baseline')\n",
    "for epoch in range(start_epoch, args.epoch):\n",
    "    train_baseline(epoch, trainloader, net, CELoss, optimizer)\n",
    "    train_loss, train_acc, test_loss, test_acc = performance(epoch, net, CELoss)\n",
    "    writer.add_scalar('train_loss', train_loss, global_step = epoch)\n",
    "    writer.add_scalar('train_accuracy', train_acc, global_step = epoch)\n",
    "    writer.add_scalar('test_loss', test_loss, global_step = epoch)\n",
    "    writer.add_scalar('test_accuracy', test_acc, global_step = epoch)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    if train_acc == 100:\n",
    "        break\n",
    "\n",
    "writer.close()\n",
    "torch.save(net, './models/baseline.pt')\n",
    "del net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(pred, y_a, y_b, lam):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "def train_mixup(epoch, trainloader, net, mixup_criterion, optimizer, alpha):\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha, use_cuda)\n",
    "        inputs, targets_a, targets_b = map(Variable, (inputs, targets_a, targets_b))\n",
    "        outputs = net(inputs)\n",
    "        loss = mixup_criterion(outputs, targets_a, targets_b, lam)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process for Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim_alpha = Real(low=1.0, high=5.0, prior='uniform', name='alpha')\n",
    "dimensions = [dim_alpha]\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness_mixup(alpha):\n",
    "    start_epoch = 0\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    net = ResNet18()\n",
    "    net.cuda()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.decay)\n",
    "    writer = SummaryWriter(f'./performance/mixup_alpha={np.round(alpha,2)}')\n",
    "    for epoch in range(start_epoch, args.epoch):\n",
    "        train_mixup(epoch, trainloader, net, mixup_criterion, optimizer, alpha)\n",
    "        train_loss, train_acc, test_loss, test_acc = performance(epoch, net, CELoss)\n",
    "        writer.add_scalar('train_loss', train_loss, global_step = epoch)\n",
    "        writer.add_scalar('train_accuracy', train_acc, global_step = epoch)\n",
    "        writer.add_scalar('test_loss', test_loss, global_step = epoch)\n",
    "        writer.add_scalar('test_accuracy', test_acc, global_step = epoch)\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        if train_acc == 100:\n",
    "            break\n",
    "    writer.close()\n",
    "    torch.save(net, f'./models/mixup_alpha={np.round(alpha,2)}.pt')\n",
    "    del net\n",
    "    return -float(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.477 | Train Acc: 58.200% (29100/50000) \t Test  Loss: 1.287 | Test  Acc: 56.770% (5677/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.731 | Train Acc: 94.300% (47150/50000) \t Test  Loss: 0.559 | Test  Acc: 85.980% (8598/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.279 | Train Acc: 99.462% (49731/50000) \t Test  Loss: 0.473 | Test  Acc: 87.140% (8714/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.192 | Train Acc: 99.822% (49911/50000) \t Test  Loss: 0.441 | Test  Acc: 88.190% (8819/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.424 | Train Acc: 60.672% (30336/50000) \t Test  Loss: 1.258 | Test  Acc: 59.280% (5928/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.877 | Train Acc: 92.394% (46197/50000) \t Test  Loss: 0.606 | Test  Acc: 84.420% (8442/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.469 | Train Acc: 98.782% (49391/50000) \t Test  Loss: 0.516 | Test  Acc: 87.430% (8743/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.371 | Train Acc: 99.550% (49775/50000) \t Test  Loss: 0.489 | Test  Acc: 88.090% (8809/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.377 | Train Acc: 60.166% (30083/50000) \t Test  Loss: 1.248 | Test  Acc: 58.010% (5801/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.928 | Train Acc: 92.734% (46367/50000) \t Test  Loss: 0.631 | Test  Acc: 84.660% (8466/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.455 | Train Acc: 98.738% (49369/50000) \t Test  Loss: 0.512 | Test  Acc: 87.110% (8711/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.370 | Train Acc: 99.514% (49757/50000) \t Test  Loss: 0.486 | Test  Acc: 88.080% (8808/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.393 | Train Acc: 61.392% (30696/50000) \t Test  Loss: 1.248 | Test  Acc: 60.060% (6006/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.753 | Train Acc: 93.420% (46710/50000) \t Test  Loss: 0.587 | Test  Acc: 84.280% (8428/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.285 | Train Acc: 99.414% (49707/50000) \t Test  Loss: 0.477 | Test  Acc: 87.220% (8722/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.239 | Train Acc: 99.816% (49908/50000) \t Test  Loss: 0.458 | Test  Acc: 88.250% (8825/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.285 | Train Acc: 62.494% (31247/50000) \t Test  Loss: 1.202 | Test  Acc: 60.970% (6097/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.628 | Train Acc: 95.868% (47934/50000) \t Test  Loss: 0.539 | Test  Acc: 86.040% (8604/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.244 | Train Acc: 99.798% (49899/50000) \t Test  Loss: 0.459 | Test  Acc: 87.740% (8774/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.230 | Train Acc: 99.920% (49960/50000) \t Test  Loss: 0.455 | Test  Acc: 88.300% (8830/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.665 | Train Acc: 56.982% (28491/50000) \t Test  Loss: 1.383 | Test  Acc: 56.150% (5615/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.875 | Train Acc: 92.896% (46448/50000) \t Test  Loss: 0.622 | Test  Acc: 84.380% (8438/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.483 | Train Acc: 98.950% (49475/50000) \t Test  Loss: 0.537 | Test  Acc: 87.050% (8705/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.417 | Train Acc: 99.576% (49788/50000) \t Test  Loss: 0.510 | Test  Acc: 88.060% (8806/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.440 | Train Acc: 58.532% (29266/50000) \t Test  Loss: 1.277 | Test  Acc: 57.040% (5704/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.371 | Train Acc: 96.642% (48321/50000) \t Test  Loss: 0.475 | Test  Acc: 85.870% (8587/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.095 | Train Acc: 99.912% (49956/50000) \t Test  Loss: 0.412 | Test  Acc: 87.990% (8799/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.085 | Train Acc: 99.976% (49988/50000) \t Test  Loss: 0.402 | Test  Acc: 88.590% (8859/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.498 | Train Acc: 58.142% (29071/50000) \t Test  Loss: 1.285 | Test  Acc: 56.650% (5665/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.901 | Train Acc: 92.002% (46001/50000) \t Test  Loss: 0.607 | Test  Acc: 84.940% (8494/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.515 | Train Acc: 98.338% (49169/50000) \t Test  Loss: 0.534 | Test  Acc: 86.970% (8697/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.435 | Train Acc: 99.188% (49594/50000) \t Test  Loss: 0.513 | Test  Acc: 87.560% (8756/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.498 | Train Acc: 60.736% (30368/50000) \t Test  Loss: 1.293 | Test  Acc: 59.180% (5918/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.543 | Train Acc: 95.156% (47578/50000) \t Test  Loss: 0.511 | Test  Acc: 85.250% (8525/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.203 | Train Acc: 99.728% (49864/50000) \t Test  Loss: 0.439 | Test  Acc: 87.460% (8746/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.178 | Train Acc: 99.906% (49953/50000) \t Test  Loss: 0.431 | Test  Acc: 88.150% (8815/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.457 | Train Acc: 62.342% (31171/50000) \t Test  Loss: 1.271 | Test  Acc: 60.940% (6094/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.970 | Train Acc: 92.150% (46075/50000) \t Test  Loss: 0.661 | Test  Acc: 83.870% (8387/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.436 | Train Acc: 98.858% (49429/50000) \t Test  Loss: 0.500 | Test  Acc: 87.700% (8770/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.402 | Train Acc: 99.556% (49778/50000) \t Test  Loss: 0.504 | Test  Acc: 88.130% (8813/10000)\n"
     ]
    }
   ],
   "source": [
    "gp_result_mixup = gp_minimize(func=fitness_mixup, dimensions=dimensions, n_calls=10, noise= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.133162</td>\n",
       "      <td>88.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.080251</td>\n",
       "      <td>88.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.686689</td>\n",
       "      <td>88.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.530056</td>\n",
       "      <td>88.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.592202</td>\n",
       "      <td>88.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.460635</td>\n",
       "      <td>88.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.186081</td>\n",
       "      <td>88.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.451972</td>\n",
       "      <td>87.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.496010</td>\n",
       "      <td>88.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.522516</td>\n",
       "      <td>88.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha  accuracy\n",
       "0  2.133162     88.19\n",
       "1  3.080251     88.09\n",
       "2  3.686689     88.08\n",
       "3  2.530056     88.25\n",
       "4  1.592202     88.30\n",
       "5  3.460635     88.06\n",
       "6  1.186081     88.59\n",
       "7  4.451972     87.56\n",
       "8  1.496010     88.15\n",
       "9  3.522516     88.13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(gp_result_mixup.x_iters, columns = [\"alpha\"]),\n",
    "(pd.Series(np.round(gp_result_mixup.func_vals*-1,2), name=\"accuracy\"))], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "    \n",
    "def train_cutmix(epoch, trainloader, net, criterion, optimizer, alpha, prob):\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        r = np.random.rand(1)\n",
    "        if alpha > 0 and r < prob:\n",
    "            # generate mixed sample\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            rand_index = torch.randperm(input.size()[0]).cuda()\n",
    "            target_a = target\n",
    "            target_b = target[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
    "            input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "            # compute output\n",
    "            output = net(input)\n",
    "            loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
    "        else:\n",
    "            # compute output\n",
    "            output = net(input)\n",
    "            loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()#反向传播\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process for Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim_alpha = Real(low=1.0, high=5.0, name='alpha')\n",
    "dim_prob = Real(low=0.1, high=0.9, name='prob')\n",
    "dimensions = [dim_alpha, dim_prob]\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness_cutmix(alpha, prob):\n",
    "    start_epoch = 0\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    net = ResNet18()\n",
    "    net.cuda()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.decay)\n",
    "    writer = SummaryWriter(f'./performance/cutmix_alpha={np.round(alpha,2)}_prob={np.round(prob,2)}')\n",
    "    for epoch in range(start_epoch, args.epoch):\n",
    "        train_cutmix(epoch, trainloader, net, CELoss, optimizer, alpha, prob)\n",
    "        train_loss, train_acc, test_loss, test_acc = performance(epoch, net, CELoss)\n",
    "        writer.add_scalar('train_loss', train_loss, global_step = epoch)\n",
    "        writer.add_scalar('train_accuracy', train_acc, global_step = epoch)\n",
    "        writer.add_scalar('test_loss', test_loss, global_step = epoch)\n",
    "        writer.add_scalar('test_accuracy', test_acc, global_step = epoch)\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        if train_acc == 100:\n",
    "            break\n",
    "    writer.close()\n",
    "    torch.save(net, f'./models/cutmix_alpha={np.round(alpha,2)}_prob={np.round(prob,2)}.pt')\n",
    "    del net\n",
    "    return -float(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\tan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "D:\\tan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.594 | Train Acc: 54.780% (27390/50000) \t Test  Loss: 1.351 | Test  Acc: 53.060% (5306/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.570 | Train Acc: 93.596% (46798/50000) \t Test  Loss: 0.483 | Test  Acc: 85.760% (8576/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.150 | Train Acc: 99.638% (49819/50000) \t Test  Loss: 0.410 | Test  Acc: 87.980% (8798/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.077 | Train Acc: 99.924% (49962/50000) \t Test  Loss: 0.379 | Test  Acc: 88.840% (8884/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.348 | Train Acc: 57.540% (28770/50000) \t Test  Loss: 1.236 | Test  Acc: 56.170% (5617/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.384 | Train Acc: 96.328% (48164/50000) \t Test  Loss: 0.447 | Test  Acc: 86.160% (8616/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.050 | Train Acc: 99.978% (49989/50000) \t Test  Loss: 0.361 | Test  Acc: 88.780% (8878/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.027 | Train Acc: 99.992% (49996/50000) \t Test  Loss: 0.351 | Test  Acc: 89.310% (8931/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.169 | Train Acc: 62.274% (31137/50000) \t Test  Loss: 1.143 | Test  Acc: 60.740% (6074/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.152 | Train Acc: 98.274% (49137/50000) \t Test  Loss: 0.450 | Test  Acc: 85.580% (8558/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.017 | Train Acc: 99.988% (49994/50000) \t Test  Loss: 0.423 | Test  Acc: 87.560% (8756/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.012 | Train Acc: 99.998% (49999/50000) \t Test  Loss: 0.388 | Test  Acc: 88.480% (8848/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.029 | Train Acc: 63.736% (31868/50000) \t Test  Loss: 1.076 | Test  Acc: 62.200% (6220/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.094 | Train Acc: 99.020% (49510/50000) \t Test  Loss: 0.507 | Test  Acc: 84.720% (8472/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.014 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.431 | Test  Acc: 86.950% (8695/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.119 | Train Acc: 61.080% (30540/50000) \t Test  Loss: 1.110 | Test  Acc: 59.720% (5972/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.174 | Train Acc: 98.522% (49261/50000) \t Test  Loss: 0.429 | Test  Acc: 86.160% (8616/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.017 | Train Acc: 99.982% (49991/50000) \t Test  Loss: 0.400 | Test  Acc: 87.790% (8779/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.011 | Train Acc: 99.996% (49998/50000) \t Test  Loss: 0.384 | Test  Acc: 88.460% (8846/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 1.988 | Train Acc: 64.298% (32149/50000) \t Test  Loss: 1.053 | Test  Acc: 62.760% (6276/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.117 | Train Acc: 98.702% (49351/50000) \t Test  Loss: 0.447 | Test  Acc: 85.770% (8577/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.013 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.404 | Test  Acc: 87.910% (8791/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.247 | Train Acc: 62.564% (31282/50000) \t Test  Loss: 1.186 | Test  Acc: 59.930% (5993/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.457 | Train Acc: 94.788% (47394/50000) \t Test  Loss: 0.454 | Test  Acc: 85.700% (8570/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.084 | Train Acc: 99.894% (49947/50000) \t Test  Loss: 0.382 | Test  Acc: 88.240% (8824/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.056 | Train Acc: 99.976% (49988/50000) \t Test  Loss: 0.372 | Test  Acc: 88.910% (8891/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.414 | Train Acc: 56.218% (28109/50000) \t Test  Loss: 1.263 | Test  Acc: 54.790% (5479/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.426 | Train Acc: 95.696% (47848/50000) \t Test  Loss: 0.456 | Test  Acc: 85.880% (8588/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.072 | Train Acc: 99.936% (49968/50000) \t Test  Loss: 0.388 | Test  Acc: 88.160% (8816/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.050 | Train Acc: 99.994% (49997/50000) \t Test  Loss: 0.378 | Test  Acc: 88.430% (8843/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 1.999 | Train Acc: 64.614% (32307/50000) \t Test  Loss: 1.067 | Test  Acc: 62.420% (6242/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.080 | Train Acc: 98.942% (49471/50000) \t Test  Loss: 0.483 | Test  Acc: 85.740% (8574/10000)\n",
      "\n",
      "Epoch: 19 ===============================================================================\n",
      "Train Loss: 0.012 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.414 | Test  Acc: 87.540% (8754/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.105 | Train Acc: 61.068% (30534/50000) \t Test  Loss: 1.105 | Test  Acc: 59.930% (5993/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.201 | Train Acc: 97.772% (48886/50000) \t Test  Loss: 0.432 | Test  Acc: 86.030% (8603/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.017 | Train Acc: 99.976% (49988/50000) \t Test  Loss: 0.401 | Test  Acc: 87.860% (8786/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.018 | Train Acc: 99.992% (49996/50000) \t Test  Loss: 0.387 | Test  Acc: 88.010% (8801/10000)\n"
     ]
    }
   ],
   "source": [
    "gp_result_cutmix = gp_minimize(func=fitness_cutmix, dimensions=dimensions, n_calls=10, noise= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>prob</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.237344</td>\n",
       "      <td>0.875980</td>\n",
       "      <td>88.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.221129</td>\n",
       "      <td>0.642483</td>\n",
       "      <td>89.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.711712</td>\n",
       "      <td>0.395033</td>\n",
       "      <td>88.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.987254</td>\n",
       "      <td>0.125719</td>\n",
       "      <td>86.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.879877</td>\n",
       "      <td>0.316913</td>\n",
       "      <td>88.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.213505</td>\n",
       "      <td>0.297492</td>\n",
       "      <td>87.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.250954</td>\n",
       "      <td>0.832802</td>\n",
       "      <td>88.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.563398</td>\n",
       "      <td>0.811996</td>\n",
       "      <td>88.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.303336</td>\n",
       "      <td>0.222179</td>\n",
       "      <td>87.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.421829</td>\n",
       "      <td>0.452589</td>\n",
       "      <td>88.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha      prob  accuracy\n",
       "0  2.237344  0.875980     88.84\n",
       "1  4.221129  0.642483     89.31\n",
       "2  2.711712  0.395033     88.48\n",
       "3  4.987254  0.125719     86.95\n",
       "4  4.879877  0.316913     88.46\n",
       "5  3.213505  0.297492     87.91\n",
       "6  4.250954  0.832802     88.91\n",
       "7  1.563398  0.811996     88.43\n",
       "8  3.303336  0.222179     87.54\n",
       "9  1.421829  0.452589     88.01"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(gp_result_cutmix.x_iters, columns = [\"alpha\",\"prob\"]),\n",
    "(pd.Series(np.round(gp_result_cutmix.func_vals*-1,2), name=\"accuracy\"))], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "    \n",
    "    input: tensor of img\n",
    "    output: tensor\n",
    "    \n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "        cut_prob: the probability of img to be cut out\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length,cut_prob=1):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "        self.cut_prob = cut_prob\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        r = np.random.random(1)\n",
    "        if r > self.cut_prob:\n",
    "            return img\n",
    "        \n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img\n",
    "    \n",
    "def cut_out(n_hole,length,prob=1):\n",
    "    '''\n",
    "    n_hole: Cutout hyparameter\n",
    "    length: Cutout hyparameter\n",
    "    prob: Cutout hyparameter\n",
    "    \n",
    "    input: Cutout hyparameter\n",
    "    output: transforms.transform type (content : totensor + cutout)\n",
    "    '''\n",
    "    cutout = Cutout(n_hole,length,prob)\n",
    "    return transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    cutout,\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process for Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim_num_hole = Integer(low=1, high=5, name='num_hole')\n",
    "dim_cut_len = Integer(low=1, high=16, name='cut_len')\n",
    "dim_cut_prob = Real(low=0.1, high=0.9, prior='uniform', name='cut_prob')\n",
    "dimensions = [dim_num_hole, dim_cut_len, dim_cut_prob]\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness_cutout(num_hole, cut_len, cut_prob):\n",
    "\n",
    "    transform_cutout = cut_out(n_hole=num_hole, length=cut_len, prob=cut_prob)\n",
    "    trainset_cutout = datasets.CIFAR10(root = './data',train = True, download = False,transform = transform_cutout)\n",
    "    trainloader_cotout = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    train_cutout = train_baseline\n",
    "    start_epoch = 0\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    net = ResNet18()\n",
    "    net.cuda()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.decay)\n",
    "    writer = SummaryWriter(f'./performance/cutout_hole={num_hole}_len={cut_len}_prob={np.round(cut_prob,2)}')\n",
    "    for epoch in range(start_epoch, args.epoch):\n",
    "        train_cutout(epoch, trainloader_cotout, net, CELoss, optimizer)\n",
    "        train_loss, train_acc, test_loss, test_acc = performance(epoch, net, CELoss)\n",
    "        writer.add_scalar('train_loss', train_loss, global_step = epoch)\n",
    "        writer.add_scalar('train_accuracy', train_acc, global_step = epoch)\n",
    "        writer.add_scalar('test_loss', test_loss, global_step = epoch)\n",
    "        writer.add_scalar('test_accuracy', test_acc, global_step = epoch)\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        if train_acc == 100:\n",
    "            break\n",
    "    writer.close()\n",
    "    torch.save(net, f'./models/cutout_hole={num_hole}_len={cut_len}_prob={np.round(cut_prob,2)}.pt')\n",
    "    del net\n",
    "    return -float(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.214 | Train Acc: 59.520% (29760/50000) \t Test  Loss: 1.171 | Test  Acc: 57.740% (5774/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.133 | Train Acc: 97.688% (48844/50000) \t Test  Loss: 0.660 | Test  Acc: 83.630% (8363/10000)\n",
      "\n",
      "Epoch: 15 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.912 | Test  Acc: 86.560% (8656/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.147 | Train Acc: 60.624% (30312/50000) \t Test  Loss: 1.124 | Test  Acc: 59.240% (5924/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.134 | Train Acc: 97.650% (48825/50000) \t Test  Loss: 0.626 | Test  Acc: 83.870% (8387/10000)\n",
      "\n",
      "Epoch: 16 ===============================================================================\n",
      "Train Loss: 0.000 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.932 | Test  Acc: 86.750% (8675/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 1.963 | Train Acc: 63.538% (31769/50000) \t Test  Loss: 1.054 | Test  Acc: 62.290% (6229/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.111 | Train Acc: 98.122% (49061/50000) \t Test  Loss: 0.670 | Test  Acc: 83.690% (8369/10000)\n",
      "\n",
      "Epoch: 12 ===============================================================================\n",
      "Train Loss: 0.002 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.858 | Test  Acc: 85.770% (8577/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.137 | Train Acc: 60.568% (30284/50000) \t Test  Loss: 1.160 | Test  Acc: 58.270% (5827/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.095 | Train Acc: 98.406% (49203/50000) \t Test  Loss: 0.650 | Test  Acc: 83.660% (8366/10000)\n",
      "\n",
      "Epoch: 14 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.939 | Test  Acc: 86.200% (8620/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 1.991 | Train Acc: 63.484% (31742/50000) \t Test  Loss: 1.068 | Test  Acc: 62.010% (6201/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.152 | Train Acc: 97.430% (48715/50000) \t Test  Loss: 0.722 | Test  Acc: 83.100% (8310/10000)\n",
      "\n",
      "Epoch: 13 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.802 | Test  Acc: 86.290% (8629/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.022 | Train Acc: 62.722% (31361/50000) \t Test  Loss: 1.085 | Test  Acc: 60.770% (6077/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.125 | Train Acc: 97.914% (48957/50000) \t Test  Loss: 0.668 | Test  Acc: 83.150% (8315/10000)\n",
      "\n",
      "Epoch: 15 ===============================================================================\n",
      "Train Loss: 0.000 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.975 | Test  Acc: 86.120% (8612/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.077 | Train Acc: 62.678% (31339/50000) \t Test  Loss: 1.112 | Test  Acc: 61.230% (6123/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.107 | Train Acc: 98.170% (49085/50000) \t Test  Loss: 0.674 | Test  Acc: 83.730% (8373/10000)\n",
      "\n",
      "Epoch: 12 ===============================================================================\n",
      "Train Loss: 0.002 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.810 | Test  Acc: 85.930% (8593/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.446 | Train Acc: 54.130% (27065/50000) \t Test  Loss: 1.276 | Test  Acc: 53.300% (5330/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.129 | Train Acc: 97.818% (48909/50000) \t Test  Loss: 0.623 | Test  Acc: 83.570% (8357/10000)\n",
      "\n",
      "Epoch: 17 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.984 | Test  Acc: 86.340% (8634/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 3.404 | Train Acc: 34.760% (17380/50000) \t Test  Loss: 1.755 | Test  Acc: 34.810% (3481/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.616 | Train Acc: 89.218% (44609/50000) \t Test  Loss: 0.600 | Test  Acc: 80.160% (8016/10000)\n",
      "\n",
      "Epoch: 20 ===============================================================================\n",
      "Train Loss: 0.037 | Train Acc: 99.428% (49714/50000) \t Test  Loss: 1.072 | Test  Acc: 82.470% (8247/10000)\n",
      "\n",
      "Epoch: 30 ===============================================================================\n",
      "Train Loss: 0.002 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 1.078 | Test  Acc: 83.740% (8374/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.247 | Train Acc: 58.552% (29276/50000) \t Test  Loss: 1.214 | Test  Acc: 55.910% (5591/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.108 | Train Acc: 98.096% (49048/50000) \t Test  Loss: 0.634 | Test  Acc: 84.070% (8407/10000)\n",
      "\n",
      "Epoch: 14 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.930 | Test  Acc: 86.300% (8630/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.012 | Train Acc: 63.932% (31966/50000) \t Test  Loss: 1.071 | Test  Acc: 62.030% (6203/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.087 | Train Acc: 98.528% (49264/50000) \t Test  Loss: 0.676 | Test  Acc: 83.890% (8389/10000)\n",
      "\n",
      "Epoch: 12 ===============================================================================\n",
      "Train Loss: 0.002 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.827 | Test  Acc: 86.380% (8638/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.254 | Train Acc: 58.870% (29435/50000) \t Test  Loss: 1.187 | Test  Acc: 57.480% (5748/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.127 | Train Acc: 97.738% (48869/50000) \t Test  Loss: 0.665 | Test  Acc: 83.600% (8360/10000)\n",
      "\n",
      "Epoch: 14 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.907 | Test  Acc: 86.080% (8608/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.184 | Train Acc: 59.908% (29954/50000) \t Test  Loss: 1.146 | Test  Acc: 58.560% (5856/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.121 | Train Acc: 97.940% (48970/50000) \t Test  Loss: 0.662 | Test  Acc: 83.840% (8384/10000)\n",
      "\n",
      "Epoch: 17 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.960 | Test  Acc: 86.260% (8626/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.144 | Train Acc: 60.598% (30299/50000) \t Test  Loss: 1.128 | Test  Acc: 59.610% (5961/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.098 | Train Acc: 98.292% (49146/50000) \t Test  Loss: 0.683 | Test  Acc: 84.010% (8401/10000)\n",
      "\n",
      "Epoch: 16 ===============================================================================\n",
      "Train Loss: 0.000 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.920 | Test  Acc: 86.530% (8653/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 1.968 | Train Acc: 63.930% (31965/50000) \t Test  Loss: 1.062 | Test  Acc: 61.980% (6198/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.116 | Train Acc: 97.982% (48991/50000) \t Test  Loss: 0.689 | Test  Acc: 83.580% (8358/10000)\n",
      "\n",
      "Epoch: 12 ===============================================================================\n",
      "Train Loss: 0.003 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.689 | Test  Acc: 86.280% (8628/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 1.917 | Train Acc: 65.010% (32505/50000) \t Test  Loss: 1.021 | Test  Acc: 63.980% (6398/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.125 | Train Acc: 97.854% (48927/50000) \t Test  Loss: 0.736 | Test  Acc: 82.990% (8299/10000)\n",
      "\n",
      "Epoch: 14 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.995 | Test  Acc: 86.090% (8609/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.072 | Train Acc: 61.618% (30809/50000) \t Test  Loss: 1.133 | Test  Acc: 59.030% (5903/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.098 | Train Acc: 98.264% (49132/50000) \t Test  Loss: 0.641 | Test  Acc: 84.050% (8405/10000)\n",
      "\n",
      "Epoch: 13 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.871 | Test  Acc: 86.560% (8656/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 2.109 | Train Acc: 61.100% (30550/50000) \t Test  Loss: 1.122 | Test  Acc: 59.570% (5957/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.110 | Train Acc: 98.108% (49054/50000) \t Test  Loss: 0.669 | Test  Acc: 84.000% (8400/10000)\n",
      "\n",
      "Epoch: 16 ===============================================================================\n",
      "Train Loss: 0.001 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.929 | Test  Acc: 86.590% (8659/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 1.936 | Train Acc: 64.838% (32419/50000) \t Test  Loss: 1.042 | Test  Acc: 62.710% (6271/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.102 | Train Acc: 98.218% (49109/50000) \t Test  Loss: 0.674 | Test  Acc: 83.670% (8367/10000)\n",
      "\n",
      "Epoch: 13 ===============================================================================\n",
      "Train Loss: 0.002 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.715 | Test  Acc: 86.240% (8624/10000)\n",
      "\n",
      "Epoch: 0 ===============================================================================\n",
      "Train Loss: 1.960 | Train Acc: 64.540% (32270/50000) \t Test  Loss: 1.050 | Test  Acc: 62.880% (6288/10000)\n",
      "\n",
      "Epoch: 10 ===============================================================================\n",
      "Train Loss: 0.097 | Train Acc: 98.416% (49208/50000) \t Test  Loss: 0.668 | Test  Acc: 83.570% (8357/10000)\n",
      "\n",
      "Epoch: 16 ===============================================================================\n",
      "Train Loss: 0.000 | Train Acc: 100.000% (50000/50000) \t Test  Loss: 0.929 | Test  Acc: 86.050% (8605/10000)\n"
     ]
    }
   ],
   "source": [
    "gp_result_cutout = gp_minimize(func=fitness_cutout, dimensions=dimensions, n_calls=20, noise= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_hole</th>\n",
       "      <th>cut_len</th>\n",
       "      <th>cut_prob</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.139495</td>\n",
       "      <td>86.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.583584</td>\n",
       "      <td>86.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.633652</td>\n",
       "      <td>85.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.294497</td>\n",
       "      <td>86.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.376475</td>\n",
       "      <td>86.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.800425</td>\n",
       "      <td>86.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.430647</td>\n",
       "      <td>85.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>86.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.245405</td>\n",
       "      <td>83.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.646275</td>\n",
       "      <td>86.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>86.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.589159</td>\n",
       "      <td>86.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.860753</td>\n",
       "      <td>86.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.586881</td>\n",
       "      <td>86.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.582053</td>\n",
       "      <td>86.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.690806</td>\n",
       "      <td>86.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481239</td>\n",
       "      <td>86.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.556681</td>\n",
       "      <td>86.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.352527</td>\n",
       "      <td>86.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.193108</td>\n",
       "      <td>86.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_hole  cut_len  cut_prob  accuracy\n",
       "0          1       12  0.139495     86.56\n",
       "1          2        3  0.583584     86.75\n",
       "2          5       10  0.633652     85.77\n",
       "3          2        9  0.294497     86.20\n",
       "4          1       14  0.376475     86.29\n",
       "5          3        8  0.800425     86.12\n",
       "6          4        4  0.430647     85.93\n",
       "7          2        7  0.131100     86.34\n",
       "8          3        6  0.245405     83.74\n",
       "9          2       12  0.646275     86.30\n",
       "10         2        1  0.900000     86.38\n",
       "11         2        1  0.589159     86.08\n",
       "12         2        6  0.860753     86.26\n",
       "13         2        4  0.586881     86.53\n",
       "14         2        5  0.582053     86.28\n",
       "15         2        3  0.690806     86.09\n",
       "16         2        3  0.481239     86.56\n",
       "17         2        3  0.556681     86.59\n",
       "18         2        3  0.352527     86.24\n",
       "19         1       10  0.193108     86.05"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(gp_result_cutout.x_iters, columns = [\"num_hole\",\"cut_len\",\"cut_prob\"]),\n",
    "(pd.Series(np.round(gp_result_cutout.func_vals*-1,2), name=\"accuracy\"))], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
